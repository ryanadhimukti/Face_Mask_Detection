{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The Rise of Face Mask Detection with Convolutional Neural Networks (CNNs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Background**\n",
    "The global COVID-19 pandemic has catalyzed notable advancements in computer vision, particularly within the domain of face mask detection. Convolutional Neural Networks (CNNs), a sophisticated form of deep learning architecture, have emerged as an efficacious tool for accurately identifying individuals wearing masks in both images and videos.\n",
    "### **Prioritizing Public Health**\n",
    "The capacity to automatically detect face masks serves a pivotal public health function. By deploying CNN-based detection systems in public areas, authorities can:\n",
    "- Monitor adherence to mask-wearing mandates\n",
    "- Identify potential health risks\n",
    "- Enhance contact tracing efforts\n",
    "\n",
    "### **The Power of CNNs**\n",
    "CNNs are remarkably proficient at extracting salient features from visual data, such as images. Their architecture, comprising convolutional layers, pooling layers, and fully connected layers, enables them to discern intricate patterns and relationships within the data. In the context of face mask detection, CNNs are trained on extensive datasets of images annotated as \"masked\" or \"unmasked.\" This training empowers them to effectively distinguish between faces with and without masks in novel images.\n",
    "### **Key Advantages of CNNs**\n",
    "CNNs offer several compelling advantages for face mask detection:\n",
    "- **High Accuracy**: CNNs can achieve impressive accuracy levels, often exceeding 95%, in identifying faces and their mask status.\n",
    "- **Scalability**: They can be readily scaled to manage large volumes of images and video streams.\n",
    "- **Real-Time Processing**: Modern CNN implementations facilitate real-time detection, rendering them suitable for time-sensitive applications.\n",
    "\n",
    "### **Beyond the Pandemic**\n",
    "The applications of face mask detection using CNNs extend well beyond the immediate public health crisis. This technology holds promise for:\n",
    "- Access control systems\n",
    "- Workplace safety monitoring\n",
    "- Surveillance and security applications\n",
    "\n",
    "### **The Future of Face Mask Detection**\n",
    "As CNNs continue to evolve and datasets grow richer, face mask detection is poised to become even more accurate, robust, and versatile. This technology has the potential to play a significant role in safeguarding public health and promoting safety measures across various domains.\n",
    "\n",
    "In conclusion, the integration of CNN-based face mask detection systems represents a significant stride towards leveraging artificial intelligence for public health and safety. As we look to the future, the continued refinement and application of these technologies promise to enhance our capacity to respond to health crises and ensure secure environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem Statement: Accurate and Efficient Face Mask Detection using Convolutional Neural Networks (CNNs)**\n",
    "The COVID-19 pandemic has necessitated the widespread adoption of face masks as a critical measure to curb viral transmission. Ensuring consistent and proper mask usage, however, presents a formidable challenge. Manual monitoring is labor-intensive and susceptible to human error. To address this challenge, we propose an automated face mask detection system leveraging Convolutional Neural Networks (CNNs).\n",
    "### **Challenges:**\n",
    "- **Accuracy**: The system must reliably distinguish between masked and unmasked individuals, even under varying lighting conditions, poses, and mask types.\n",
    "- **Real-Time Performance**: For practical deployments in public spaces and surveillance contexts, the system must process video streams efficiently with minimal latency.\n",
    "- **Scalability**: The system should scale to handle large volumes of images and video data without compromising performance.\n",
    "- **Generalizability**: The model must generalize across diverse populations and mask designs to ensure broad applicability.\n",
    "\n",
    "### **Our Approach:**\n",
    "We propose a CNN-based face mask detection system harnessing the capabilities of deep learning for precise and efficient mask identification. The system will be trained on a meticulously curated dataset, encompassing a wide array of lighting conditions, poses, ethnicities, and mask types. We will employ advanced techniques to optimize the CNN architecture, ensuring real-time performance and scalability while maintaining high accuracy.\n",
    "### **Significance:**\n",
    "An accurate and efficient face mask detection system using CNNs can substantially bolster public health efforts by:\n",
    "- Enhancing adherence to mask-wearing mandates\n",
    "- Enabling real-time monitoring in public spaces\n",
    "- Facilitating contact tracing\n",
    "- Alleviating the burden on manual monitoring personnel\n",
    "\n",
    "### **Expected Outcomes:**\n",
    "We aim to develop a CNN-based face mask detection system that achieves:\n",
    "- **High Accuracy**: Attaining accuracy levels exceeding 95% in identifying masked and unmasked individuals.\n",
    "- **Real-Time Processing**: Ensuring real-time processing capabilities for video streams.\n",
    "- **Scalability**: Scaling to support large-scale deployments.\n",
    "- **Generalizability**: Generalizing effectively to diverse scenarios and mask designs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importing Essential Libraries**\n",
    "The following Python libraries are indispensable for this face mask detection project utilizing Convolutional Neural Networks (CNNs):\n",
    "* **numpy (np):** This library offers robust numerical computing capabilities, which are essential for manipulating and processing image data used in CNN training.\n",
    "* **pandas (pd):** Pandas provides comprehensive data analysis and manipulation tools, facilitating the handling of annotations or labels associated with the image data.\n",
    "* **matplotlib (mpl) and matplotlib.pyplot (plt):** These libraries enable sophisticated data visualization, which is crucial for exploring the image dataset, visualizing training progress, and analyzing results.\n",
    "* **os:** The os library affords interaction with the operating system, which is vital for loading image data from specific directories or file paths.\n",
    "\n",
    "These libraries constitute the foundational framework for our face mask detection project leveraging CNNs, ensuring efficient data manipulation, analysis, and visualization throughout the development process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Leveraging TensorFlow for Deep Learning**\n",
    "This section introduces the TensorFlow library, an exceptionally powerful framework for building and training deep learning models, including Convolutional Neural Networks (CNNs). In the context of face mask detection:\n",
    "\n",
    "- **import tensorflow as tf**: This command imports the TensorFlow library and assigns it the alias `tf` for convenience. This alias allows us to access TensorFlow's extensive functionalities seamlessly throughout the code.\n",
    "- **from tensorflow import keras**: This command imports the Keras API, a high-level interface built atop TensorFlow, designed explicitly for constructing and training neural networks. Keras simplifies the processes of defining, compiling, and training CNN models for face mask detection.\n",
    "\n",
    "TensorFlow and Keras will serve as the primary deep learning tools for developing and training our CNN-based face mask detection system, ensuring an efficient and streamlined approach to leveraging advanced neural network capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Defining Data Directories**\n",
    "The following code block delineates the directories housing our face mask dataset, segmented for training, validation, and testing purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'D:\\sucofindo\\pelatihan\\CNN\\Final_Assignment_Face Mask Dataset\\Train'\n",
    "validation_dir = r'D:\\sucofindo\\pelatihan\\CNN\\Final_Assignment_Face Mask Dataset\\Validation'\n",
    "test_dir =r'D:\\sucofindo\\pelatihan\\CNN\\Final_Assignment_Face Mask Dataset\\Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Augmenting Data with ImageDataGenerator**\n",
    "This line of code imports the `ImageDataGenerator` class from the `tensorflow.keras.preprocessing.image` module. In the context of face mask detection using Convolutional Neural Networks (CNNs):\n",
    "- **ImageDataGenerator**: This class serves as a sophisticated tool for augmenting image datasets. Data augmentation entails the artificial creation of variations in existing images, thereby enriching the training data and enhancing the model's generalization capabilities.\n",
    "\n",
    "By leveraging various transformations through `ImageDataGenerator`, we can achieve the following:\n",
    "- **Increase Dataset Size and Diversity**: Augment the training dataset's size and diversity without necessitating additional data collection.\n",
    "- **Enhance Model Robustness**: Improve the model's robustness by mitigating the risk of overfitting to specific image characteristics.\n",
    "\n",
    "For instance, `ImageDataGenerator` can apply the following transformations:\n",
    "- **Random Flips**: Horizontal and vertical flips.\n",
    "- **Random Rotations**: Rotations by random angles.\n",
    "- **Random Zooms**: Random zooming in and out.\n",
    "- **Color Jittering**: Adjustments in brightness, contrast, and saturation.\n",
    "\n",
    "These transformations enable the model to learn to recognize faces and masks under varied conditions, ultimately leading to improved performance in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preparing Training and Validation Data**\n",
    "This code block demonstrates the process of preparing our training and validation data using ImageDataGenerator for our face mask detection Convolutional Neural Network (CNN) model.\n",
    "\n",
    "## **Data Generators:**\n",
    "Two `ImageDataGenerator` objects are instantiated:\n",
    "- **train_datagen**: Utilized for preprocessing training images.\n",
    "- **test_datagen**: Utilized for preprocessing validation images, treated similarly to test data in this context.\n",
    "\n",
    "## **Rescaling:**\n",
    "Both generators are configured with `rescale=1./255`, normalizing pixel values in images from the 0-255 range to the 0-1 range. This normalization is a common preprocessing step in deep learning models to ensure consistent input values.\n",
    "## **Flow from Directory:**\n",
    "- **train_generator**: Created using `train_datagen.flow_from_directory` to read images from the `train_dir` directory.\n",
    "    - **target_size=(128, 128)**: Resizes images to a uniform size of 128x128 pixels.\n",
    "    - **batch_size=20**: Groups images into batches of 20 for training the CNN model.\n",
    "    - **class_mode='binary'**: As we are performing binary classification (masked vs. unmasked), `class_mode` is set to 'binary'.\n",
    "- **validation_generator**: Created similarly to `train_generator` to read images from the `validation_dir` directory with identical parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(128, 128),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(128, 128),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output confirms that the `flow_from_directory` function successfully identified:\n",
    "\n",
    "- 10,000 images in the `train_dir` directory, categorized into two classes (likely masked and unmasked).\n",
    "- 800 images in the `validation_dir` directory, categorized into two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importing Essential Building Blocks: Layers and Models**\n",
    "This section introduces the fundamental components required for constructing our CNN model for face mask detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **from tensorflow.keras import layers**: This command imports the `layers` module from TensorFlow's Keras API. This module encompasses a diverse array of pre-defined layers such as convolutional layers, pooling layers, and activation functions, which constitute the essential elements of Convolutional Neural Networks (CNNs).\n",
    "- **from tensorflow.keras import models**: This command imports the `models` module from TensorFlow's Keras API. This module enables us to define the overall architecture of our CNN model by sequentially stacking the layers imported from the `layers` module.\n",
    "\n",
    "## **Application in Face Mask Detection:**\n",
    "In the context of face mask detection, we will utilize various layers from the layers module to perform critical operations, including:\n",
    "- **Feature Extraction**: Convolutional layers will be employed to extract features from images, identifying patterns such as edges and textures.\n",
    "- **Dimensionality Reduction**: Pooling layers will be used to reduce the dimensionality of the data, thereby decreasing computational complexity while retaining essential information.\n",
    "- **Non-Linearity Introduction**: Activation functions will introduce non-linearities, allowing the network to model complex relationships within the data.\n",
    "- **Classification**: Fully connected layers will classify images as \"masked\" or \"unmasked.\"\n",
    "\n",
    "By meticulously combining these layers in a specific architecture using the `models` module, we can construct a robust CNN model capable of accurate face mask detection. This strategic assembly of layers ensures that the model efficiently learns and generalizes from the training data, ultimately enhancing its performance in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Constructing the CNN Model Architecture**\n",
    "The following code block outlines the architecture of our Convolutional Neural Network (CNN) model designed for face mask detection. Utilizing the Sequential model API from Keras, we will define the sequence of layers comprising the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# First convolutional block\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(128, 128, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Second convolutional block\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Third convolutional block\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Fourth convolutional block\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten layer\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Dense layer for classification\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Breakdown of the Architecture:**\n",
    "- **Sequential Model**: We define a sequential model using `models.Sequential()`. This API allows for the sequential addition of layers to construct the CNN architecture.\n",
    "\n",
    "- **Convolutional Blocks**: The model incorporates several convolutional blocks, each consisting of:\n",
    "\n",
    "    - **Convolutional Layer (Conv2D)**: Utilizes filters of size (3, 3) to extract features from the input image. The number of filters (32, 64, 128) determines the number of feature maps learned at each layer.\n",
    "    - **Activation Layer (ReLU)**: Introduces non-linearity into the network, enabling it to learn more complex patterns.\n",
    "    - **Pooling Layer (MaxPooling2D)**: Reduces the dimensionality of the data by downsampling the feature maps, thereby enhancing performance and reducing computational cost.\n",
    "- **Flatten Layer**: This layer transforms the multi-dimensional feature maps into a one-dimensional vector, making the data suitable for the final dense layer.\n",
    "\n",
    "- **Dense Layer**: This fully connected layer, with 512 neurons and a ReLU activation function, performs the final classification task.\n",
    "\n",
    "- **Output Layer**: The concluding dense layer contains one neuron with a sigmoid activation function. Given that we are performing binary classification (masked vs. unmasked), the sigmoid function outputs a probability between 0 and 1, where a value closer to 1 indicates \"masked\" and a value closer to 0 indicates \"unmasked.\"\n",
    "\n",
    "By meticulously combining these layers in the specified architecture, we construct a robust CNN model capable of accurate face mask detection. This architectural design ensures the efficient learning and generalization from the training data, ultimately enhancing the model's performance in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Compiling the CNN Model**\n",
    "This section elucidates the compilation step for our CNN model, designed for face mask detection. Compilation configures the model's learning process by specifying the loss function, optimizer, and performance metrics to track during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Importing the Optimizer**: We import the `optimizers` module from TensorFlow's Keras API to access various optimization algorithms.\n",
    "- **Compilation Process**: The `model.compile` method configures the training process:\n",
    "    - **Loss Function (binary_crossentropy)**: Given that we are performing binary classification (masked vs. unmasked), this function measures the discrepancy between the predicted probabilities and the true labels (0 for unmasked, 1 for masked). The model strives to minimize this loss during training.\n",
    "    - **Optimizer (RMSprop)**: This optimization algorithm adjusts the model's weights based on the calculated loss. In this instance, we employ the RMSprop optimizer with a learning rate of 1×10^−4, a hyperparameter that controls the step size during weight updates.\n",
    "    - **Metrics (acc)**: We specify 'acc' (accuracy) as the metric to monitor the model's performance during training and validation. Accuracy denotes the proportion of correctly classified images (masked vs. unmasked).\n",
    "\n",
    "By configuring these parameters, we establish the foundation for the model's learning process, ensuring it is well-equipped to optimize performance and achieve high accuracy in detecting face masks. This meticulous setup is crucial for guiding the model toward effective training and validation, ultimately enhancing its real-world applicability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training the CNN Model**\n",
    "This section delineates the training process for our CNN model dedicated to face mask detection. The code trains the model on the prepared training data (`train_generator`) and evaluates its performance on the validation data (`validation_generator`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m473s\u001b[0m 919ms/step - acc: 0.8654 - loss: 0.3129 - val_acc: 1.0000 - val_loss: 0.0261\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.0000e+00 - loss: 0.0000e+00 - val_acc: 1.0000 - val_loss: 0.0425\n",
      "Epoch 3/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 281ms/step - acc: 0.9758 - loss: 0.0738 - val_acc: 1.0000 - val_loss: 0.0256\n",
      "Epoch 4/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - acc: 0.0000e+00 - loss: 0.0000e+00 - val_acc: 1.0000 - val_loss: 0.0233\n",
      "Epoch 5/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 251ms/step - acc: 0.9856 - loss: 0.0416 - val_acc: 1.0000 - val_loss: 0.0141\n",
      "Epoch 6/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329us/step - acc: 0.0000e+00 - loss: 0.0000e+00 - val_acc: 1.0000 - val_loss: 0.0024\n",
      "Epoch 7/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 255ms/step - acc: 0.9882 - loss: 0.0318 - val_acc: 1.0000 - val_loss: 0.0065\n",
      "Epoch 8/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - acc: 0.0000e+00 - loss: 0.0000e+00 - val_acc: 1.0000 - val_loss: 0.0019\n",
      "Epoch 9/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 254ms/step - acc: 0.9909 - loss: 0.0274 - val_acc: 1.0000 - val_loss: 0.0146\n",
      "Epoch 10/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - acc: 0.0000e+00 - loss: 0.0000e+00 - val_acc: 0.9750 - val_loss: 0.0207\n",
      "Epoch 11/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 257ms/step - acc: 0.9926 - loss: 0.0207 - val_acc: 1.0000 - val_loss: 0.0103\n",
      "Epoch 12/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - acc: 0.0000e+00 - loss: 0.0000e+00 - val_acc: 1.0000 - val_loss: 0.0106\n",
      "Epoch 13/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 258ms/step - acc: 0.9937 - loss: 0.0169 - val_acc: 0.9500 - val_loss: 0.1049\n",
      "Epoch 14/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332us/step - acc: 0.0000e+00 - loss: 0.0000e+00 - val_acc: 1.0000 - val_loss: 7.0104e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 250ms/step - acc: 0.9923 - loss: 0.0221 - val_acc: 1.0000 - val_loss: 0.0072\n",
      "Epoch 16/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - acc: 0.0000e+00 - loss: 0.0000e+00 - val_acc: 1.0000 - val_loss: 0.0067\n",
      "Epoch 17/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 262ms/step - acc: 0.9953 - loss: 0.0136 - val_acc: 1.0000 - val_loss: 7.9858e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - acc: 0.0000e+00 - loss: 0.0000e+00 - val_acc: 1.0000 - val_loss: 0.0016\n",
      "Epoch 19/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 253ms/step - acc: 0.9957 - loss: 0.0117 - val_acc: 1.0000 - val_loss: 0.0076\n",
      "Epoch 20/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298us/step - acc: 0.0000e+00 - loss: 0.0000e+00 - val_acc: 1.0000 - val_loss: 0.0174\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=500,\n",
    "      epochs=20,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Code Explanation:**\n",
    "- **model.fit**: This method initiates the training process.\n",
    "- **train_generator**: Specifies the training data generator that provides batches of images and labels during training.\n",
    "- **steps_per_epoch=500**: Defines the number of batches to iterate through per epoch, where an epoch represents one complete pass through the training data.\n",
    "- **epochs=20**: Sets the total number of training epochs.\n",
    "- **validation_data=validation_generator**: Specifies the validation data generator used to evaluate the model's performance after each epoch.\n",
    "- **validation_steps=2**: Defines the number of validation batches to use for evaluation during each epoch.\n",
    "\n",
    "## **Output Interpretation:**\n",
    "The output showcases the training progress over 20 epochs. Key elements include:\n",
    "- **Epoch (e.g., Epoch 1/20)**: Indicates the current epoch number out of the total.\n",
    "- **Progress Within an Epoch (e.g., 500/500)**: Shows progress within an epoch, indicating the current batch being processed (e.g., 500th out of 500 batches).\n",
    "- **Time per Step (e.g., 919ms/step)**: The average time taken to process each batch.\n",
    "- **Training Accuracy (acc: 0.8654)**: Represents the percentage of correctly classified images within the current batch.\n",
    "- **Training Loss (loss: 0.3129)**: Measures how well the model's predictions align with the true labels for the current batch.\n",
    "- **Validation Accuracy (val_acc: 1.0000)**: Indicates the model's performance on the validation data after each epoch.\n",
    "- **Validation Loss (val_loss: 0.0261)**: Measures the model's loss on the validation data after each epoch.\n",
    "\n",
    "By meticulously monitoring these metrics, we can assess the model's learning progress and its ability to generalize to unseen data. The detailed tracking of accuracy and loss, both during training and validation, ensures that the model is effectively learning to distinguish between masked and unmasked faces, thereby enhancing its real-world applicability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preserving the Trained Model**\n",
    "The following line of code employs the `model.save` method to preserve the trained CNN model, ensuring its availability for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_cnn_project_P1.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **model.save**: This function, part of the Keras API, facilitates the serialization of the model's architecture, weights, and optimizer configuration into a file.\n",
    "- **\"model_cnn_project_P1.keras\"**: This parameter specifies the filename and path for saving the model. Here, the model is saved as \"model_cnn_project_P1.keras\", with the `.keras` extension, commonly used for Keras models.\n",
    "\n",
    "## **Benefits of Saving the Model:**\n",
    "- **Reusability**: Saving the trained model allows for future predictions on new data without the need to retrain, conserving both time and computational resources.\n",
    "- **Deployment**: For real-world application deployment, the saved model can be loaded into the deployment environment, ready to make predictions.\n",
    "## **Future Considerations**:\n",
    "- **Version Control**: Integrating the saved model into a version control system (e.g., Git) enables tracking changes and reverting to previous versions if necessary.\n",
    "- **File Format**: Depending on deployment requirements and compatibility with other tools, alternative model saving formats such as HDF5 (.h5) or TensorFlow SavedModel (.pb) can be explored.\n",
    "\n",
    "By preserving the trained model, we ensure its longevity and readiness for various future applications, enhancing its practical utility and deployment efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cleaning Up Resources After Training**\n",
    "This section outlines the essential steps to clean up resources following the training of your CNN model for face mask detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K \n",
    "\n",
    "K.clear_session()\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Importing Backend**: We import the `backend` module as `K` from TensorFlow's Keras API. This module provides low-level functionality to interact with the computational backend, such as the CPU or GPU.\n",
    "- **Clearing the Session (K.clear_session())**: This step is crucial for proper resource management. TensorFlow employs sessions to manage computations and allocate memory for models and data. After training, invoking `K.clear_session()` explicitly terminates the current session, releasing any resources held by the model and backend. This prevents memory leaks and ensures efficient resource utilization, especially when conducting multiple training sessions or working with large models.\n",
    "- **Deleting the Model (del model)**: This command explicitly deletes the model object from memory. Following the session clearance, this step ensures that all model-related resources are fully released.\n",
    "\n",
    "## **Importance of Cleanup**:\n",
    "- **Memory Management**: Clearing the session and deleting the model object aids in preventing memory leaks and ensures efficient memory usage. This is particularly important when dealing with large models or sequentially training multiple models.\n",
    "- **Resource Management**: TensorFlow sessions can retain computational resources, such as GPUs. Clearing the session allows these resources to be freed and made available for other processes as needed.\n",
    "\n",
    "By following these cleanup steps, we ensure that our computational resources are managed effectively, promoting optimal performance and preventing unnecessary resource contention in subsequent operations. This meticulous approach is vital for maintaining system stability and efficiency in deep learning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Augmentation for Training Robust CNNs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(128, 128),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(128, 128),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Code Explanation:**\n",
    "### **Augmentation Techniques:**\n",
    "- **rotation_range=40**: Randomly rotates images by up to 40 degrees.\n",
    "- **width_shift_range=0.2, height_shift_range=0.2**: Randomly shifts images horizontally or vertically by up to 20% of their width or height, respectively.\n",
    "- **shear_range=0.2**: Applies a shearing transformation to slightly distort the images.\n",
    "- **zoom_range=0.2**: Randomly zooms in or out of images by up to 20%.\n",
    "- **horizontal_flip=True**: Randomly flips images horizontally with a 50% probability.\n",
    "\n",
    "## **Output Interpretation**\n",
    "This confirms that the `flow_from_directory` function correctly identified the specified number of images in the training and validation directories (10,000 and 800 images, respectively), distributed across two classes (presumably masked and unmasked).\n",
    "\n",
    "By employing these data augmentation techniques, we enhance the diversity of our training dataset, thereby improving the robustness and performance of our CNN model in real-world face mask detection scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Architecture (using the Sequential API)**\n",
    "### **Detailed Breakdown:**\n",
    "\n",
    "**Dropout Layer**: A `Dropout(0.5)` layer is included to randomly drop 50% of the neurons during training, preventing overfitting by reducing dependency on specific neurons.\n",
    "\n",
    "This comprehensive construction and compilation of the CNN model ensures it is well-prepared for the task of face mask detection, with robust layers for feature extraction, effective regularization techniques, and an efficient optimization strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# Convolutional Blocks with Max Pooling\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(128, 128, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten Layer\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Dropout for Regularization\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Dense Layers for Classification\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Model Compilation\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training the CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 472ms/step - acc: 0.8166 - loss: 0.4089 - val_acc: 0.9422 - val_loss: 0.1694\n",
      "Epoch 2/10\n",
      "\u001b[1m 13/300\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 383ms/step - acc: 0.9137 - loss: 0.2677"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - acc: 0.9135 - loss: 0.2546 - val_acc: 0.9438 - val_loss: 0.1572\n",
      "Epoch 3/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 458ms/step - acc: 0.9105 - loss: 0.2363 - val_acc: 0.9469 - val_loss: 0.1390\n",
      "Epoch 4/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - acc: 0.9041 - loss: 0.2480 - val_acc: 0.9688 - val_loss: 0.0977\n",
      "Epoch 5/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 458ms/step - acc: 0.9236 - loss: 0.2035 - val_acc: 0.9703 - val_loss: 0.1053\n",
      "Epoch 6/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - acc: 0.9499 - loss: 0.1478 - val_acc: 0.9875 - val_loss: 0.0742\n",
      "Epoch 7/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 466ms/step - acc: 0.9320 - loss: 0.1854 - val_acc: 0.9719 - val_loss: 0.0919\n",
      "Epoch 8/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - acc: 0.9283 - loss: 0.1741 - val_acc: 0.9625 - val_loss: 0.1271\n",
      "Epoch 9/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 452ms/step - acc: 0.9368 - loss: 0.1715 - val_acc: 0.9672 - val_loss: 0.1070\n",
      "Epoch 10/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - acc: 0.9390 - loss: 0.1680 - val_acc: 0.9438 - val_loss: 0.1354\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=300,\n",
    "      epochs=10,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Leveraging a Pre-trained Model for Feature Extraction**\n",
    "This section demonstrates the incorporation of a pre-trained convolutional neural network (CNN), specifically VGG19, to enhance our face mask detection model by utilizing its feature extraction capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the VGG19 Model Architecture:\n",
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "#Transfer Learning with VGG19:\n",
    "conv_base = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(128, 128, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Code Explanation**\n",
    "- `VGG19`: This import statement brings the VGG19 model architecture from the Keras applications module into our codebase.\n",
    "- `VGG19(weights='imagenet', ...)`: This line of code initializes the VGG19 model.\n",
    "- `weights='imagenet'`: Specifies that the model should load pre-trained weights derived from the ImageNet dataset. These weights encapsulate valuable features learned from a large and diverse set of images.\n",
    "- `include_top=False`: By setting this parameter to `False`, we exclude the fully connected layers at the top of the VGG19 model, which are typically used for classifying images into 1000 categories. For our face mask detection task, we are interested only in the convolutional layers that perform feature extraction.\n",
    "- `input_shape=(128, 128, 3)`: Defines the input shape of the images expected by the model, which in this case are 128x128 pixels with three color channels (RGB).\n",
    "\n",
    "## **Benefits of Transfer Learning**\n",
    "**Reduced Training Time**: Utilizing the pre-trained weights of VGG19 substantially reduces the training time compared to training a model from scratch. The convolutional layers of VGG19 have already been trained to recognize intricate patterns and features from a vast amount of data, thus providing a solid foundation for our model.\n",
    "\n",
    "**Improved Performance**: Transfer learning often enhances the model's performance, especially when the source task (ImageNet classification) is closely related to the target task (face mask detection). The pre-trained weights serve as an excellent starting point, allowing the model to focus on learning task-specific features effectively.\n",
    "\n",
    "By leveraging the pre-trained VGG19 model, we can harness the power of advanced image features learned from the extensive ImageNet dataset, thereby optimizing our face mask detection model for better accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building the Final Classifier on Top of Pre-trained Features**\n",
    "In this section, we demonstrate the process of constructing a robust classifier by integrating pre-trained features from the VGG19 model for the task of face mask detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base) # Pre-trained VGG19 model for feature extraction\n",
    "model.add(layers.Flatten()) # Flatten the extracted features\n",
    "model.add(layers.Dense(256, activation='relu')) # Dense layer with ReLU activation\n",
    "model.add(layers.Dense(1, activation='sigmoid')) # Output layer for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Advantages of this Approach**\n",
    "**Leveraging Pre-trained Features**: By building upon the VGG19 model, which has been pre-trained on the extensive ImageNet dataset, we harness powerful feature representations that significantly enhance our model's ability to generalize well to new data.\n",
    "\n",
    "**Efficient Training**: Utilizing a pre-trained model reduces the time and computational resources required for training. The pre-trained convolutional layers already contain valuable information, allowing the model to converge faster and achieve better performance with fewer training epochs.\n",
    "\n",
    "**Enhanced Performance**: Transfer learning often leads to superior performance in specific tasks, such as face mask detection, as the pre-trained model provides a solid foundation that can be fine-tuned for our target application.\n",
    "\n",
    "By constructing our classifier on top of the pre-trained VGG19 features, we create a powerful and efficient model capable of accurately distinguishing between masked and unmasked faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Compiling the Model for Face Mask Detection**\n",
    "This section details the configuration of the learning process for our Convolutional Neural Network (CNN) model, which has been built upon the pre-trained VGG19 architecture to classify faces as masked or unmasked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(learning_rate=2e-5),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizer (RMSprop)**: The `RMSprop` optimizer, imported from Keras optimizers, is selected to fine-tune our model. The optimizer adjusts the model weights during training to minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Advantages of this Compilation Approach**\n",
    "**Balanced Learning**: The combination of `binary_crossentropy` as the loss function and `RMSprop` with a carefully chosen learning rate ensures a balanced approach to learning. The model can fine-tune its parameters effectively, leveraging the rich feature set provided by the pre-trained VGG19 layers.\n",
    "\n",
    "**Robust Optimization**: The `RMSprop` optimizer, with its adaptive learning rate, facilitates robust optimization by adjusting the learning rate for each parameter, enhancing the model’s ability to converge to an optimal solution.\n",
    "\n",
    "**Performance Monitoring**: Utilizing accuracy as the primary metric allows for straightforward and meaningful performance monitoring, ensuring that the model is progressing in the right direction during the training and validation phases.\n",
    "\n",
    "By meticulously configuring the compilation settings, we set the stage for an effective training process, poised to yield a highly accurate and efficient model for face mask detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Saving Model Checkpoints During Training**\n",
    "This section elucidates the process of incorporating a ModelCheckpoint callback to periodically save the model during the training phase of our face mask detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"CNN_Final_Project_Model-{epoch:02d}.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ModelCheckpoint**: This class, part of the Keras callbacks module, facilitates the checkpointing of models during training. It ensures that the model's state is periodically saved to disk.\n",
    "\n",
    "**Detailed Breakdown**:\n",
    "\n",
    "- **Callback Creation**:\n",
    "- `checkpoint_cb = keras.callbacks.ModelCheckpoint(\"CNN_Final_Project_Model-{epoch:02d}.keras\")`: This line instantiates the `ModelCheckpoint` callback. The argument specifies the file path pattern for saving the models.\n",
    "- **File Path Pattern**:\n",
    "- **Base Filename**: `CNN_Final_Project_Model-`: This is the base part of the filename for all saved models.\n",
    "- **Epoch Number Formatting**: `{epoch:02d}`: This placeholder is dynamically replaced with the current epoch number, zero-padded to two digits. This ensures that each saved model file is uniquely named according to its epoch.\n",
    "- **File Extension**: `.keras`: This denotes the Keras model file format, indicating the saved file contains a serialized Keras model.\n",
    "\n",
    "## **Benefits of Model Checkpoints**\n",
    "**Fault Tolerance**:\n",
    "\n",
    "- **Recovery from Interruptions**: By saving model checkpoints periodically, you can resume training from the latest saved state if the process is interrupted. This is crucial in avoiding the need to restart training from scratch in case of hardware failures or power outages.\n",
    "\n",
    "**Experiment Comparison**:\n",
    "\n",
    "- **Performance Evaluation**: Saving models at various points during training (e.g., after each epoch) allows for comparative analysis of their performance on validation data. This aids in selecting the best-performing model by examining different checkpoints.\n",
    "\n",
    "**Early Stopping Integration**:\n",
    "\n",
    "- **Optimization**: Model checkpoints can be effectively combined with early stopping techniques. If the validation performance plateaus or deteriorates for a predefined number of epochs, training can be halted, and the best model checkpoint can be loaded for further use. This approach ensures efficient training without overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Implementing in Training Workflow**\n",
    "Integrating the `ModelCheckpoint` callback into the training process ensures that the model's progress is safeguarded and can be utilized for evaluation and potential continuation. Here's an example of incorporating it into the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4063s\u001b[0m 14s/step - acc: 0.9239 - loss: 0.1751 - val_acc: 1.0000 - val_loss: 5.6800e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 544ms/step - acc: 0.9974 - loss: 0.0113 - val_acc: 1.0000 - val_loss: 1.1660e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3820s\u001b[0m 13s/step - acc: 0.9909 - loss: 0.0304 - val_acc: 0.9984 - val_loss: 0.0038\n",
      "Epoch 4/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 552ms/step - acc: 0.9953 - loss: 0.0178 - val_acc: 0.9937 - val_loss: 0.0095\n",
      "Epoch 5/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3819s\u001b[0m 13s/step - acc: 0.9939 - loss: 0.0169 - val_acc: 1.0000 - val_loss: 0.0013\n",
      "Epoch 6/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4571s\u001b[0m 15s/step - acc: 0.9925 - loss: 0.0312 - val_acc: 1.0000 - val_loss: 0.0039\n",
      "Epoch 7/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3872s\u001b[0m 13s/step - acc: 0.9953 - loss: 0.0160 - val_acc: 1.0000 - val_loss: 7.7367e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 559ms/step - acc: 1.0000 - loss: 6.8833e-04 - val_acc: 0.9937 - val_loss: 0.0253\n",
      "Epoch 9/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3815s\u001b[0m 13s/step - acc: 0.9960 - loss: 0.0139 - val_acc: 0.9984 - val_loss: 0.0046\n",
      "Epoch 10/10\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 558ms/step - acc: 0.9976 - loss: 0.0122 - val_acc: 1.0000 - val_loss: 4.2277e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=300,\n",
    "      epochs=10,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=20,\n",
    "      callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, `checkpoint_cb` is passed as an element of the `callbacks` list to the `model.fit` method. This setup ensures that the model's state is saved after each epoch according to the specified file path pattern.\n",
    "\n",
    "By meticulously integrating model checkpointing, we bolster the training process with robust recovery mechanisms, facilitate comprehensive performance analysis, and optimize training efficiency through early stopping strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluating Model Performance on Unseen Data**\n",
    "This section illustrates the method for assessing the trained Convolutional Neural Network (CNN) model's performance on a separate test dataset, ensuring its efficacy in face mask detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 992 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(128, 128),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output confirms that the `flow_from_directory` function has successfully located 992 images within the `test_dir` directory, distributed across two classes (presumably 'masked' and 'unmasked'), mirroring the structure of the training and validation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluation Process**\n",
    "The configured test data generator (`test_generator`) is now poised to evaluate the trained model's performance. This is achieved through the `model.evaluate` function, which computes key metrics such as accuracy and loss on the unseen test data. These metrics provide valuable insights into the model's generalization capabilities, indicating how well it is likely to perform on real-world, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 3s/step - acc: 0.9988 - loss: 0.0038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0038883087690919638, 0.9989919066429138]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_generator, steps=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Test Lost, Test Accuracy]\n",
    "\n",
    "**Evaluation Method**: The `model.evaluate` function is called with the `test_generator`, which yields the following:\n",
    "\n",
    "**Test Loss**: A measure of how well the model's predictions align with the true labels, computed on the test dataset.\n",
    "**Test Accuracy**: The proportion of correctly classified images in the test dataset, reflecting the model's overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Benefits of Evaluation on Unseen Data**\n",
    "- **Validation of Generalization**: Evaluating on a separate test dataset, which the model has not encountered during training, provides a robust estimate of its generalization ability. This helps in understanding the model's performance in real-world scenarios.\n",
    "- **Model Improvement**: Insights gained from the evaluation can guide further improvements and refinements to the model, such as adjusting hyperparameters or incorporating additional data augmentation techniques.\n",
    "- **Confidence in Deployment**: A thorough evaluation ensures that the model is reliable and performant, instilling confidence in its deployment for practical applications.\n",
    "\n",
    "By rigorously evaluating the CNN model on unseen test data, we ensure its robustness and readiness for real-world face mask detection tasks, ultimately contributing to the development of effective and reliable AI solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusion**\n",
    "\n",
    "This document comprehensively explored the development and evaluation of a Convolutional Neural Network (CNN) model for classifying faces as masked or unmasked. We leveraged techniques like data augmentation and transfer learning to enhance the model's performance.\n",
    "\n",
    "## **Key Points**:\n",
    "\n",
    "- The code effectively demonstrates the process of creating data generators for training, validation, and testing data, ensuring proper image preprocessing and batching.\n",
    "- The model architecture utilizes a pre-trained VGG19 model for feature extraction, followed by a custom head for the binary classification task.\n",
    "- The model is compiled with an appropriate loss function (binary cross-entropy), optimizer (RMSprop), and metrics (accuracy) for face mask detection.\n",
    "- Model checkpoints are implemented to save the model state periodically during training, enabling fault tolerance and experiment comparison.\n",
    "- The test data generator prepares unseen data for evaluating the model's generalizability on a separate dataset.\n",
    "\n",
    "## **Future Directions**:\n",
    "- **Data Augmentation and Hyperparameter Tuning**: Experimentation with varied data augmentation techniques and hyperparameter tuning may further enhance the model's performance and resilience.\n",
    "- **Exploration of Alternative Models**: Investigating other pre-trained models or CNN architectures could potentially yield improved results for this specific dataset, offering a broader spectrum of performance benchmarks.\n",
    "- **Handling Class Imbalance**: Incorporating techniques to address class imbalance, especially if there is a significant disparity between the number of masked and unmasked images, could lead to more balanced and accurate model predictions.\n",
    "- **Deployment**: Deploying the trained model to an appropriate platform, such as a mobile application or a web-based interface, will enable real-world face mask detection, extending the model's utility beyond the confines of this study.\n",
    "\n",
    "By adhering to these strategies, we aim to enhance the effectiveness, reliability, and applicability of the CNN model for face mask detection, ultimately contributing to the development of robust AI solutions for public health and safety."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
